# Model Selection for AI-Powered Tools

**Feature:** Language Model Selection for Sampling  
**Status:** âœ… Implemented  
**Version:** 1.5.0  
**Last Updated:** 2025-10-02

## Overview

The Enhanced ADO MCP Server's AI-powered tools now support **model selection** via VS Code's language model sampling API. By default, all intelligent tools are configured to use **free models** (GPT-4o mini, GPT-4.1, GPT-4o) to avoid rate limits and ensure reliability.

## Default Model Preferences

All AI-powered tools now automatically request:

```typescript
{
  hints: [
    { name: 'gpt-4o-mini' },  // FREE: GPT-4o mini for speed (0x tokens)
    { name: 'gpt-4.1' },       // FREE: GPT-4.1 balanced (0x tokens)
    { name: 'gpt-4o' },        // FREE: GPT-4o general purpose (0x tokens)
    { name: 'gpt-3.5' },       // FREE: GPT-3.5 fallback (0x tokens)
    { name: 'mini' }           // Generic mini model fallback
  ],
  speedPriority: 0.9,          // Speed is very important
  costPriority: 0.7,           // Cost efficiency matters
  intelligencePriority: 0.5    // Moderate intelligence needed
}
```

### Why These Defaults?

- **GPT-4o mini**: Best balance of speed and capability, FREE (0x tokens)
- **GPT-4.1**: Balanced performance, FREE (0x tokens)
- **GPT-4o**: Strong general purpose model, FREE (0x tokens)
- **No paid models**: Avoids rate limiting issues (o3-mini, Claude models have usage limits)
- **Speed Priority (0.9)**: Prioritizes fast response times for interactive use
- **Cost Priority (0.7)**: Efficient token usage for bulk operations
- **Intelligence Priority (0.5)**: Sufficient capability for structured analysis tasks

## Available Models in VS Code

Based on your GitHub Copilot subscription, you may have access to:

### Fast Models (Recommended for Most Tasks)
- **GPT-4o mini** (0x tokens) - Best for speed
- **GPT-3.5 mini** (0x tokens) - Fast baseline
- **o3-mini** (0.33x tokens) - Efficient reasoning
- **o4-mini (Preview)** (0.33x tokens) - Latest fast model

### Balanced Models
- **GPT-4.1** (0x tokens) - Good balance
- **GPT-4o** (0x tokens) - Strong general purpose
- **Claude Sonnet 3.5** (1x tokens) - Excellent analysis

### High-Capability Models (Slower, More Expensive)
- **Claude Sonnet 3.7** (1x tokens) - Advanced reasoning
- **Claude Sonnet 4** (1x tokens) - Latest Claude
- **Claude Sonnet 4.5 (Preview)** (1x tokens) - Cutting edge
- **GPT-5** (1x tokens) - Premium
- **GPT-5-Codex (Preview)** (1x tokens) - Code-focused

## How Model Selection Works

### 1. MCP Protocol Flow

```
Enhanced ADO MCP Server
    â†“
createMessage() with modelPreferences
    â†“
VS Code MCP Client
    â†“
Model Selection Logic (matches hints + priorities)
    â†“
Selected Language Model (e.g., GPT-4o mini)
    â†“
Response back to MCP Server
```

### 2. Hint Matching

VS Code processes hints as **substrings**:
- `"gpt-4o-mini"` matches "GPT-4o mini", "gpt-4o-mini-turbo", etc.
- `"mini"` matches any model with "mini" in the name
- `"claude"` matches any Claude model

### 3. Priority Weights

VS Code considers all three priorities when selecting:
- **speedPriority**: Prefers models with lower latency
- **costPriority**: Prefers models with lower token costs
- **intelligencePriority**: Prefers models with higher capabilities

### 4. Final Selection

VS Code picks the best available model based on:
1. Your GitHub Copilot subscription tier
2. Model availability at request time
3. Hint preferences (in order)
4. Priority scores (weighted)

## Customizing Model Preferences

### Per-Tool Customization

If you want different models for specific tools, you can modify the analyzer code:

```typescript
// Example: Use more capable model for complex analysis
const result = await samplingClient.createMessage({
  systemPromptName: 'full-analyzer',
  userContent: workItemData,
  modelPreferences: {
    hints: [
      { name: 'claude-sonnet' },      // Prefer Claude Sonnet
      { name: 'gpt-4o' }              // Fallback to GPT-4o
    ],
    speedPriority: 0.5,
    intelligencePriority: 0.9         // Prioritize capability
  }
});
```

### Global Default Override

To change defaults for all tools, edit `src/utils/sampling-client.ts`:

```typescript
export function getDefaultModelPreferences(): ModelPreferences {
  return {
    hints: [
      { name: 'your-preferred-model' }
    ],
    speedPriority: 0.8,
    costPriority: 0.6,
    intelligencePriority: 0.7
  };
}
```

## Performance Comparison

Based on typical work item analysis tasks:

| Model | Avg Latency | Token Cost | Quality Score | Recommended Use |
|-------|-------------|------------|---------------|-----------------|
| GPT-4o mini | ~1-2s | 0.0x | 85/100 | âœ… Default for all tasks |
| o3-mini | ~1-2s | 0.33x | 88/100 | âœ… Complex analysis |
| GPT-3.5 mini | ~1-2s | 0.0x | 80/100 | âœ… Bulk operations |
| GPT-4o | ~3-5s | 0.0x | 92/100 | Large features |
| Claude Sonnet 3.5 | ~3-6s | 1.0x | 95/100 | Critical work items |
| GPT-5 | ~5-10s | 1.0x | 98/100 | Architecture decisions |

## Affected Tools

All AI-powered tools now use model preferences:

1. âœ… `wit-intelligence-analyzer` - Work item quality analysis
2. âœ… `wit-ai-assignment-analyzer` - AI suitability scoring

## User Experience

### Before (No Model Selection)
- Used whatever model VS Code defaulted to (usually GPT-4 or Claude Sonnet)
- Average response time: 5-10 seconds
- Higher token usage

### After (With Model Selection)
- Explicitly requests free models (GPT-4o mini, GPT-4.1, GPT-4o)
- Average response time: 1-3 seconds âš¡
- Lower token usage ðŸ’°
- More consistent performance

## Troubleshooting

### "Model not available" errors

If you see model unavailability errors:

1. **Check your GitHub Copilot subscription** - Some models require premium tiers
2. **Try fallback models** - The system automatically tries alternative models
3. **Update VS Code** - Ensure you're on version 1.87+ with latest Copilot extension

### Unexpected model selection

VS Code may select a different model than requested if:
- The requested model is unavailable
- Another model better matches the priority weights
- Rate limiting is active for the requested model

### Performance not improved

If you don't see speed improvements:
1. Verify model selection in tool output metadata (check `model` field in response)
2. Ensure network latency isn't the bottleneck
3. Check if rate limiting is affecting requests

## Future Enhancements

- [ ] Per-user model preference configuration
- [ ] Automatic model selection based on work item complexity
- [ ] Model performance tracking and optimization
- [ ] Support for local/private models
- [ ] Model cost tracking and reporting

## References

- [VS Code MCP Specification Blog](https://code.visualstudio.com/blogs/2025/06/12/full-mcp-spec-support)
- [MCP Sampling Documentation](https://modelcontextprotocol.io/docs/concepts/sampling)
- [Model Preferences Specification](https://modelcontextprotocol.io/specification/2025-06-18/client/sampling#model-preferences)

---

**Related Documentation:**
- [AI-Powered Features](../specs/AI_POWERED_FEATURES.md)
- [Sampling Implementation](../mcp_server/src/utils/sampling-client.ts)
- [Configuration Guide](../README.md#configuring-language-model-access-sampling)

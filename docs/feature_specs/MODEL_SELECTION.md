# Model Selection for AI-Powered Tools

**Feature:** Language Model Selection for Sampling  
**Status:** âœ… Implemented  
**Version:** 1.5.0  
**Last Updated:** 2025-10-07

> **See Also:**
> - **AI tools:** [AI Intelligence Tools](./AI_INTELLIGENCE_TOOLS.md), [Bulk AI Enhancement](./BULK_AI_ENHANCEMENT.md)
> - **AI overview:** [AI-Powered Features](./AI_POWERED_FEATURES.md)

## Overview

The Enhanced ADO MCP Server's AI-powered tools now support **model selection** via VS Code's language model sampling API. By default, all intelligent tools are configured to use **free models** (GPT-4o mini, GPT-4.1, GPT-4o) to avoid rate limits and ensure reliability.

## Default Model Preferences

All AI-powered tools now automatically select **free models only** using cost-based filtering:

```typescript
{
  hints: [
    { name: 'mini' },              // PATTERN: Any "mini" model (GPT-5 mini, GPT-4o mini, etc.)
    { name: 'gpt-5' },             // PATTERN: Any GPT-5 variant (future-proof)
    { name: 'gpt-4' },             // PATTERN: Any GPT-4 variant (4.1, 4o, 4-turbo, etc.)
    { name: 'gpt-3' },             // PATTERN: Any GPT-3 variant (legacy support)
    { name: 'gpt' }                // PATTERN: Any GPT model (ultimate fallback)
  ],
  costPriority: 1.0,               // CRITICAL: Strongly prefer free models (0x tokens)
  speedPriority: 0.9,              // Speed is very important (prefer "mini" models)
  intelligencePriority: 0.3        // Lower priority - free fast models are sufficient
}
```

### How Model Selection Works

**Cost-Based Filtering (The Key Innovation):**

1. **costPriority: 1.0** - VS Code filters to ONLY models with 0x token cost (free models)
2. **speedPriority: 0.9** - Among free models, prefer models with "mini" in the name
3. **Pattern hints** - Guide selection among free models: mini â†’ gpt-5 â†’ gpt-4 â†’ gpt-3 â†’ gpt

**VS Code's Selection Process:**

1. Filter models by cost (only 0x token models pass)
2. Score remaining models based on hint matching and priorities
3. Select highest-scoring free model
4. If model unavailable, try next highest-scoring free model

**Example Selection:**
- Available: GPT-5 mini (0x), GPT-4.1 (0x), GPT-4o (0x), Claude Sonnet (1x)
- Cost filter removes: Claude Sonnet (paid)
- Hint 'mini' matches: GPT-5 mini âœ… **Selected**
- If GPT-5 mini unavailable, tries: GPT-4.1 or GPT-4o (both match 'gpt-4')

### Future-Proofing with Generic Patterns

**Why generic patterns instead of specific model names?**

| Pattern | Matches | Why It's Better |
|---------|---------|----------------|
| `'mini'` | GPT-5 mini, GPT-4o mini, GPT-6 mini, etc. | Works with any future "mini" model |
| `'gpt-5'` | gpt-5-mini, gpt-5-turbo, gpt-5.1, etc. | Adapts to GPT-5 variants |
| `'gpt-4'` | gpt-4.1, gpt-4o, gpt-4-turbo, etc. | Compatible with all GPT-4 models |
| `'gpt'` | Any GPT model | Ultimate fallback |

**Benefits:**
- âœ… Works when GitHub renames models (e.g., "GPT-4.1" â†’ "GPT-4.2")
- âœ… Works when new free models are added (e.g., "GPT-6 mini")
- âœ… Works across different IDEs that may expose different model names
- âœ… No code changes needed when model catalog changes

### Why This Approach?

**Cost Priority = 1.0 (Most Important)**
- Forces VS Code to filter to ONLY 0x token (free) models
- Eliminates risk of accidentally using paid models
- Works even if paid models have better hint matches
- Ensures consistent zero-cost operation

**Generic Patterns Instead of Specific Names**
- `'mini'` - Works with any "mini" model (GPT-5 mini, GPT-4o mini, future models)
- `'gpt-5'`, `'gpt-4'`, `'gpt-3'` - Compatible with version changes and variants
- `'gpt'` - Ultimate fallback for any GPT model
- **Benefit:** Code never breaks when GitHub changes model names or adds new models

**Speed Priority = 0.9 (Among Free Models)**
- Prefers "mini" variants which are optimized for speed
- Ensures fast response times (1-3 seconds)
- Still constrained by cost priority (only free models)

**Intelligence Priority = 0.3 (Low)**
- Free models are highly capable for structured analysis
- Prioritizes speed and cost over raw intelligence
- Prevents selection of expensive high-intelligence models

**Key Benefits:**
- âœ… **IDE-agnostic** - Works in VS Code, Cursor, or any MCP client
- âœ… **Bulletproof** - Never uses paid models, even if hints change
- âœ… **Self-updating** - Adapts to new free models automatically
- âœ… **Zero maintenance** - No code changes needed when model catalog updates

## Available Models in VS Code

Based on your GitHub Copilot subscription, you may have access to:

### Fast Models (Recommended for Most Tasks)
- **GPT-5 mini** (0x tokens) - Best for speed âš¡ **CONFIGURED**
- **GPT-4.1** (0x tokens) - Automatic fallback âœ… **CONFIGURED**
- **o3-mini** (0.33x tokens) - Efficient reasoning (paid)
- **o4-mini (Preview)** (0.33x tokens) - Latest fast model (paid)

### Balanced Models
- **GPT-4o** (0x tokens) - Strong general purpose **CONFIGURED**
- **Claude Sonnet 3.5** (1x tokens) - Excellent analysis (paid)

### High-Capability Models (Slower, More Expensive)
- **Claude Sonnet 3.7** (1x tokens) - Advanced reasoning
- **Claude Sonnet 4** (1x tokens) - Latest Claude
- **Claude Sonnet 4.5 (Preview)** (1x tokens) - Cutting edge
- **GPT-5** (1x tokens) - Premium
- **GPT-5-Codex (Preview)** (1x tokens) - Code-focused

## Configuring Model Access in VS Code

### Step-by-Step: Enable Model Access

1. **Open Command Palette** (`F1` or `Ctrl+Shift+P` / `Cmd+Shift+P`)
2. Type **"MCP"** and select **"MCP: List Servers"**
3. Find **"enhanced-ado-msp"** in the server list
4. Click on the server name
5. Select **"Configure Model Access"**
6. In the model selection dialog, **check ALL free models** (any model showing **0x** tokens):
   - âœ… Any model with **0x** in the token cost column
   - Examples: GPT-5 mini, GPT-4.1, GPT-4o, or any other 0x models
   - ðŸ’¡ **Tip:** Enable all free models - the system will pick the best one automatically!
7. Click **"OK"** to save

### What Happens After Configuration

- The MCP server can now access the selected models via VS Code's sampling API
- When an AI-powered tool runs:
  1. **Cost filter** - VS Code removes all non-free (paid) models from consideration
  2. **Hint matching** - Scores remaining free models based on patterns ('mini', 'gpt-5', etc.)
  3. **Priority weighting** - Applies speed priority to prefer fast free models
  4. **Selection** - Picks the highest-scoring free model
- If the selected model is unavailable, the process repeats with the next-best free model
- **No manual fallback needed** - The system handles everything automatically
- **Works with any free models** - Present or future

### Verifying Model Access

After configuration, verify access by:

1. **Via VS Code UI:**
   - Open Command Palette (`F1`)
   - Run **"MCP: List Servers"**
   - Select **"enhanced-ado-msp"**
   - Check that models are listed with green checkmarks

2. **Via Tool Execution:**
   - Run any AI-powered tool (e.g., `wit-ai-intelligence`)
   - Check the response metadata for `model` field
   - Should show a free model name (any model you enabled with 0x tokens)
   - Example: `gpt-5-mini`, `gpt-4.1`, `gpt-4o`, or future free models

### Troubleshooting Access Issues

**Problem:** "Language model sampling is not available"

**Solutions:**
1. Ensure GitHub Copilot extension is installed and active
2. Run the configuration steps above to grant access
3. Enable **all free models** (0x tokens) in the configuration dialog
4. Restart VS Code if needed

**ðŸ’¡ Pro Tip:** When in doubt, enable ALL models showing **0x** tokens!

**Problem:** No models appear in configuration dialog

**Solutions:**
1. Verify GitHub Copilot subscription is active
2. Update VS Code to version 1.87+ (or latest Insiders)
3. Update GitHub Copilot extension to latest version

## How Model Selection Works

### 1. MCP Protocol Flow

```
Enhanced ADO MCP Server
    â†“
createMessage() with modelPreferences
    â†“
VS Code MCP Client
    â†“
Model Selection Logic (matches hints + priorities)
    â†“
Selected Language Model (e.g., GPT-4o mini)
    â†“
Response back to MCP Server
```

### 2. Hint Matching

VS Code processes hints as **substrings**:
- `"gpt-4o-mini"` matches "GPT-4o mini", "gpt-4o-mini-turbo", etc.
- `"mini"` matches any model with "mini" in the name
- `"claude"` matches any Claude model

### 3. Priority Weights

VS Code considers all three priorities when selecting:
- **speedPriority**: Prefers models with lower latency
- **costPriority**: Prefers models with lower token costs
- **intelligencePriority**: Prefers models with higher capabilities

### 4. Final Selection

VS Code picks the best available model based on:
1. Your GitHub Copilot subscription tier
2. Model availability at request time
3. Hint preferences (in order)
4. Priority scores (weighted)

## Customizing Model Preferences

### Per-Tool Customization

If you want different models for specific tools, you can modify the analyzer code:

```typescript
// Example: Use more capable model for complex analysis
const result = await samplingClient.createMessage({
  systemPromptName: 'full-analyzer',
  userContent: workItemData,
  modelPreferences: {
    hints: [
      { name: 'claude-sonnet' },      // Prefer Claude Sonnet
      { name: 'gpt-4o' }              // Fallback to GPT-4o
    ],
    speedPriority: 0.5,
    intelligencePriority: 0.9         // Prioritize capability
  }
});
```

### Global Default Override

To change defaults for all tools, edit `src/utils/sampling-client.ts`:

```typescript
export function getDefaultModelPreferences(): ModelPreferences {
  return {
    hints: [
      { name: 'your-preferred-model' }
    ],
    speedPriority: 0.8,
    costPriority: 0.6,
    intelligencePriority: 0.7
  };
}
```

## Performance Comparison

Based on typical work item analysis tasks:

| Model | Avg Latency | Token Cost | Quality Score | Recommended Use |
|-------|-------------|------------|---------------|---------------|
| GPT-5 mini | ~1-2s | 0.0x | 88/100 | âœ… Default for all tasks |
| GPT-4.1 | ~2-3s | 0.0x | 90/100 | âœ… Automatic fallback |
| GPT-4o | ~3-5s | 0.0x | 92/100 | âœ… Secondary fallback |
| o3-mini | ~1-2s | 0.33x | 88/100 | Complex analysis (paid) |
| Claude Sonnet 3.5 | ~3-6s | 1.0x | 95/100 | Critical work items (paid) |
| GPT-5 | ~5-10s | 1.0x | 98/100 | Architecture decisions (paid) |

## Affected Tools

All AI-powered tools now use model preferences:

1. âœ… `wit-ai-intelligence` - Work item quality analysis
2. âœ… `wit-ai-assignment` - AI suitability scoring

## User Experience

### Before (No Model Selection)
- Used whatever model VS Code defaulted to (usually GPT-4 or Claude Sonnet)
- Average response time: 5-10 seconds
- Higher token usage

### After (With Model Selection)
- Explicitly requests free models (GPT-4o mini, GPT-4.1, GPT-4o)
- Average response time: 1-3 seconds âš¡
- Lower token usage ðŸ’°
- More consistent performance

## Troubleshooting

### "Model not available" errors

If you see model unavailability errors:

1. **Check your GitHub Copilot subscription** - Some models require premium tiers
2. **Try fallback models** - The system automatically tries alternative models
3. **Update VS Code** - Ensure you're on version 1.87+ with latest Copilot extension

### Unexpected model selection

VS Code may select a different model than requested if:
- The requested model is unavailable
- Another model better matches the priority weights
- Rate limiting is active for the requested model

### Performance not improved

If you don't see speed improvements:
1. Verify model selection in tool output metadata (check `model` field in response)
2. Ensure network latency isn't the bottleneck
3. Check if rate limiting is affecting requests

## Future Enhancements

- [ ] Per-user model preference configuration
- [ ] Automatic model selection based on work item complexity
- [ ] Model performance tracking and optimization
- [ ] Support for local/private models
- [ ] Model cost tracking and reporting

## References

- [VS Code MCP Specification Blog](https://code.visualstudio.com/blogs/2025/06/12/full-mcp-spec-support)
- [MCP Sampling Documentation](https://modelcontextprotocol.io/docs/concepts/sampling)
- [Model Preferences Specification](https://modelcontextprotocol.io/specification/2025-06-18/client/sampling#model-preferences)

---

**Related Documentation:**
- [AI-Powered Features](../specs/AI_POWERED_FEATURES.md)
- [Sampling Implementation](../mcp_server/src/utils/sampling-client.ts)
- [Configuration Guide](../README.md#configuring-language-model-access-sampling)
